{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 4: Introduction to Reinforcement Leaerning\n",
    "\n",
    "### Overview\n",
    "In this lab you will get a small taste of the power Reinforcement Learning (RL) has to offer when it comes to robotics and control. Your task will be to use the [gymnasium](https://gymnasium.farama.org/introduction/basic_usage/) library to simulate a small MDP environment, and solve it using Tabular Q-Learning. \n",
    "\n",
    "Given the interdisciplinary nature of RL, the breadth of the topic can easily fill multiple courses. Here, you will be introduced to the math you need to implement Tabular Q-learning, but the reasoning behind all the steps is not explained in detail. I recommend starting at this [lecture](https://www.youtube.com/watch?v=E3f2Camj0Is&list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&index=2) for those interested. Note that it takes three lectures (2-4) to arrive at the Q-learning formulation we will use here.\n",
    "\n",
    "#### I. gym\n",
    "\n",
    "#### II. Markov Decision Processes\n",
    "\n",
    "#### III. Tabular Q-learning\n",
    "\n",
    "### Assignment \n",
    "There is only one task at the bottom of the lab:\n",
    "- **Question 1**: Implement Tabular Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. gym\n",
    "\n",
    "The [gym](https://gym.openai.com/docs/) library is a benchmark suite for RL provided by OpenAI. It is widely used in research due to its easy integration with python. Note that while I am using the deprecated version of gym, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we will look at a small environment simulating a taxi driving whose goal is to pick up a passenger from one of four possible locations, and drop them off at another. Read the documentation for [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/) to get familiar with the environment and understand it. Since the number of possible states is relatively small, we can find the optimal policy with a simple dynamic learning algorithm.  \n",
    "\n",
    "Let's take a look at the env:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 406\n",
      "info: {'prob': 1.0, 'action_mask': array([0, 1, 0, 0, 0, 0], dtype=int8)}\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n",
    "state, info = env.reset() \n",
    "print(f\"state: {state}\")\n",
    "print(f\"info: {info}\")\n",
    "print(env.render()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `reset()` method returns the current state of the environment (state), which is an integer encoding the state using `((taxi_row * 5 + taxi_col) * 5 + passenger_location) * 4 + destination`. There is also additional info provided, which may be of relevant use to you. \n",
    "\n",
    "Once you reset the env, the second step is to control the agent interacting with it. We can do this with env.step(), where the input should be an integer b/w 0 and 5 as described above. Once we take our desired action with env.step(), it returns 5 items: the next state, the reward received for taking our action, two booleans that checks if the episode terminated or truncated, and additional information (check the documentation for details). \n",
    "\n",
    "**Note 1**: While the action should be sampled from the set of possible actions `action = env.action_space.sample(info[\"action_mask\"])`, this may not be best when learning. Why?\n",
    "\n",
    "**Note 2**: `terminated` is a boolean that is triggered when a terminal state is reached, whereas `truncated` is triggered if we are using a timelimit wrapper to shorten the duration of the episode. Currently, the episode length is 200, so at the final step only `terminated` would trigger. For this lab, you can simply ignore the output of truncated, and assume it will always output False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: 426\n",
      "info: {'prob': 1.0, 'action_mask': array([0, 1, 1, 0, 0, 0], dtype=int8)}\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "state: 446\n",
      "reward: -1\n",
      "terminated: False\n",
      "truncated: False\n",
      "info: {'prob': 1.0, 'action_mask': array([0, 1, 0, 1, 0, 0], dtype=int8)}\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"ansi\").env\n",
    "state, info = env.reset() \n",
    "print(f\"state: {state}\")\n",
    "print(f\"info: {info}\") \n",
    "print(env.render())\n",
    "\n",
    "action = env.action_space.sample(info[\"action_mask\"])\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "print(f\"state: {state}\")\n",
    "print(f\"reward: {reward}\") \n",
    "print(f\"terminated: {terminated}\")\n",
    "print(f\"truncated: {truncated}\") \n",
    "print(f\"info: {info}\")\n",
    "print(env.render())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Markov Decision Processes\n",
    "Markov Chains (MC) are powerful tools for modeling the dynamics of systems. What if we wanted to use an MC to model a certain game, evaluating different policies to compare them? We can just use MCs, but since we want a good metric to evaluate these policies, we need to consider taking different actions and recieving a reward signal for feedback. At some state $s_t$ in the MC, we can take an action $a_t$, and observe not only the next state $s_{t+1}$, but also the reward $r_t$ associated with that transition. This is how Markov Decision Processes (MDP) are formed i.e., they combine elements of control with the Markovian model.  \n",
    "\n",
    "MDPs rely on the Markov Assumption:\n",
    "$$\n",
    "P(s_{t+1}|s_t,a_t)=P(s_{t+1}|h_t,a_t),\n",
    "$$\n",
    "where $h_t$ is the history of all states up to step $t$. Essentially, this says that we only need to know the current state and the action to predict the outcome. With this assumption, we can formulate an MDP $\\mathcal{M}:\\{\\mathcal{S},\\mathcal{A},P,R\\}$ as a collection of a set of states, a set of actions, a probability transition function and a reward transition function respectively. \n",
    "\n",
    "We also need a way to express the policy:  \n",
    "$$\\pi: \\mathcal{S}\\mapsto\\mathcal{A},$$\n",
    "where in the determenistic case $\\pi(s)=a$. Otherwise $\\pi(a|s) = Pr.(a_t=a|s_t=s)$\n",
    "\n",
    "At every state $s_t$, we receive a reward $r_t=R(s,a)$ for taking action $a_t$. By rolling out a certain policy $\\pi$ in our MDP, we can collect what we call the return at step $t$:\n",
    "$$\n",
    "G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\ldots.\n",
    "$$\n",
    "You can think of this as the total payout we will get if we run polciy $\\pi$ in our given MDP, where $\\gamma$ is a the discount factor we give towards rewards we receive in the future. We set $\\gamma<1$ in order to give higher weight to rewards that come sooner, but also this makes sure that our return $G_t$ is finite even if our policy runs for an infinite time. This way we do not have to worry about the horizon of our MDP. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value and Q Function.\n",
    "A major component in reinforcement learning is keeping track of the expected return a certain policy will provide in an environment. This is defined as the value function:\n",
    "$$\n",
    "V^{\\pi}(s_t=s)=E_{\\pi}[G_t|s_t=s].\n",
    "$$\n",
    "If we knew the value function for any policy $\\pi$, we can easily compare the quality of different polcieis. But we can go one step further with our evaluation. Say at a given state $s_t$ we wanted to evaluate our policy $\\pi$, but only after taking a certain action $a_t$, and then continuing with our policy $\\pi$. This is defined as the Q-function: \n",
    "$$\n",
    "Q^{\\pi}(s_t=s,a_t=a) = R(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}P(s'|s,a)V^{\\pi}(s')\n",
    "$$\n",
    "The methodology here is that we can now evaluate our off policy decision to take action $a$. Since we know how we can estimate the value function, computing the Q-values allows us to take $\\max_a Q^\\pi(s,a)$ instead of following the policy, since this is gauranteed to be atleast as good as following $\\pi$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Tabular Q-Learning\n",
    "In Q-Learning we measure the Temporal Difference between **target** and **current estimate** of the Q-function. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\textrm{target: }&\\qquad r_t + \\gamma\\max_{a'}Q(s_{t+1},a')\\\\\n",
    "\\textrm{estimate: }&\\qquad Q(s_t,a_t)\n",
    "\\end{align}\n",
    "$$\n",
    "Their difference measures the error between what the environment is telling us, and our current estimate. We can use this error to update our current estimate, which formulates the Q-learning algorithm:\n",
    "$$\n",
    "Q(s_t,a_t) := Q(s_t,a_t)+\\alpha\\Big(r_t+\\gamma\\max_{a'}Q(s_{t+1},a')-Q(s_t,a_t)\\Big).\n",
    "$$\n",
    "Now we just need a way to evaluate the policy, for which we use the epsilon-greedy approach: \n",
    "$$\n",
    "\\pi(a|s)=\n",
    "\\begin{cases}\n",
    "\\textrm{arg}\\max_a Q^\\pi(s,a)\\qquad \\textrm{w. prob } 1-\\epsilon,\\\\\n",
    "\\textrm{take a random action}\\qquad \\textrm{w. prob } \\epsilon.\n",
    "\\end{cases}\n",
    "$$\n",
    "Note how $\\epsilon$ controls how much we $\\textit{explore}$ the environment versus how much we $\\textit{exploit}$ our policy. A lot of RL is posed as a battle between exploration and exploitation, hence $\\epsilon$ is an important parameter. Typically we want to decrease the amount we explore as our policy gets better, and we do this by adapting some sort of **decay** to $\\epsilon$. In addition, since $\\alpha$ controls the speed at which we update our policy, you can consider adapting some decay as well. \n",
    "\n",
    "I will leave this element for you to experiment with, as it serves as an interesting challenge to try and find the correct parameters and fine tune. As a reference, you should be able to find the policy within 20,000 episodes (where episodes end when the taxi drops off the passenger.) In fact, even 5,000 is enough but you will see slight improvement following. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implement Tabular Q-Learning\n",
    "\n",
    "The actual algorithm can be written within one cell, or broken up into smaller components, it is your choice. There are only two things I require:\n",
    "\n",
    "(1) **Represent your Q-table as 2-dim numpy array**, over the state and action space. While this is not a hard requirement, it will be useful, and hence is set up in the next cell.\n",
    "\n",
    "(2) **Plot the reward of your policy (for an episode) vs the number of episodes used during training so far**. This will be your main evaluation during training, so you should structure your code to record this metric, and plot it after each run. Does your policy converge to some reasonable reward?\n",
    "\n",
    "(3) **Evaluate your final policy over 1000 episodes, and print the mean and standard deviation of the episodic return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-table\n",
    "q_table = np.random.randn(env.observation_space.n, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "# colors used\n",
    "ORANGE = '#FF9132'\n",
    "TEAL = '#0598B0'\n",
    "GREEN = '#008F00'\n",
    "PURPLE = '#8A2BE2'\n",
    "GRAY = '#969696'\n",
    "FIG_WIDTH = 4\n",
    "FIG_HEIGHT = 6\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.size\": 10,\n",
    "    \"axes.titlesize\": 10,\n",
    "    \"axes.spines.right\": False,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"lines.linewidth\": 2\n",
    "})\n",
    "\n",
    "# plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate your policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "metadata": {
   "interpreter": {
    "hash": "e14277392997ec7b8d0d4e1eacc6a12462d831e78f154a1c368aae24d965e9e8"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "3c298a6b474198d4f2a95d09a445852ca9cc03f9b965c6380c4df9c16413714d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
